{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosodic parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "STONES = ['parse_by_line', 'parse_by_window', 'parse_by_phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_path1='/Volumes/Present/DH/corpora/chadwyck_poetry/xml/african-american/beadlesa/Z200265018.xml'\n",
    "eg_path2='/Volumes/Present/DH/corpora/chadwyck_poetry/xml/english/miscell2/Z200439011.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLINES=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prosodic as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_plain(path, OK=['l','lb'], BAD=['note','edit'], body_tag='poem', line_lim=None, modernize_spelling=True):\n",
    "    from llp.corpus.chadwyck_poetry import xml2txt\n",
    "    return xml2txt(path, OK=OK,BAD=BAD,body_tag=body_tag,line_lim=line_lim,modernize_spelling=modernize_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_plain(eg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path_to_txt_or_xml_file):\n",
    "    try:\n",
    "        if path_to_txt_or_xml_file.endswith('.xml'):\n",
    "            txt=text_plain(path_to_txt_or_xml_file)\n",
    "        else:\n",
    "            with open(path_to_txt_or_xml_file) as f:\n",
    "                txt=f.read()\n",
    "        return txt\n",
    "    except IOError:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(string, sub):\n",
    "    count = start = 0\n",
    "    while True:\n",
    "        start = string.find(sub, start) + 1\n",
    "        if start > 0:\n",
    "            count+=1\n",
    "        else:\n",
    "            return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw string parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_line(line,meter,require_parse_data=True):\n",
    "    \"\"\"\n",
    "    Get data from the prosodic line object, with its meter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get phonological info\n",
    "    weight_str=line.str_weight()\n",
    "    sonority_str=line.str_sonority()\n",
    "    stress_str=line.str_stress()\n",
    "\n",
    "    # store metrical constraint stats\n",
    "    bp=line.bestParse(meter)\n",
    "    \n",
    "    # require \n",
    "    if require_parse_data:\n",
    "        if not bp:\n",
    "            return {}\n",
    "        \n",
    "    ap=line.allParses(meter)\n",
    "    output_dict={}\n",
    "    output_dict['prosodic_line']=line.txt #.encode('utf-8',errors='ignore')\n",
    "    output_dict['parse']=bp.posString(viols=True) if bp else ''\n",
    "    #output_dict['parse']=output_dict['parse'] #.encode('utf-8',errors='ignore')\n",
    "    meter_str=output_dict['meter']=bp.str_meter() if bp else ''\n",
    "    output_dict['num_parses']=len(ap)\n",
    "    output_dict['num_viols'] = bp.totalCount if bp else ''\n",
    "    output_dict['score_viols'] = bp.score() if bp else ''\n",
    "    output_dict['num_sylls']=bp.num_sylls if bp else ''\n",
    "    output_dict['num_words']=len(line.words())\n",
    "    for c in meter.constraints:\n",
    "        sumviol = sum([parse.constraintCounts[c] if c in parse.constraintCounts else 0 for parse in ap])\n",
    "        output_dict[c.name_weight+'_bestparse']=bp.constraintCounts[c] if bp and c in bp.constraintCounts else 0\n",
    "        output_dict[c.name_weight+'_allparse_sum']=sumviol if sumviol else 0\n",
    "    \n",
    "    ## store phonological constraint stats\n",
    "    output_dict['prosodic_stress']=stress_str\n",
    "    output_dict['prosodic_weight']=weight_str\n",
    "    output_dict['prosodic_sonority']=sonority_str\n",
    "    output_dict['num_monosylls']=len([w for w in line.words() if w.numSyll==1])\n",
    "    output_dict['[*clash_across]']=counts(stress_str,'P#P') + counts(stress_str,'P#S') + counts(stress_str,'S#P') + counts(stress_str,'S#S')\n",
    "    output_dict['[*clash_within]']=counts(stress_str,'PP') + counts(stress_str,'PS') + counts(stress_str,'SP') + counts(stress_str,'SS')\n",
    "    output_dict['[*clash_across_primary]']=counts(stress_str,'P#P')\n",
    "    output_dict['[*clash_within_primary]']=counts(stress_str,'PP')\n",
    "    output_dict['[*lapse_across]']=counts(stress_str,'U#U')\n",
    "    output_dict['[*lapse_within]']=counts(stress_str,'UU')\n",
    "    output_dict['[*WSP]']=0\n",
    "    output_dict['[*PEAKPROM]']=0\n",
    "    output_dict['[*High_Stress]']=0\n",
    "    output_dict['[*Low_Unstress]']=0\n",
    "    output_dict['[*High_Strong]']=0\n",
    "    output_dict['[*Low_Weak]']=0\n",
    "    for s,w,hml,mtr in zip(stress_str,weight_str,sonority_str,meter_str):\n",
    "        if s=='U' and w=='H':\n",
    "            output_dict['[*WSP]']+=1\n",
    "        if (s=='P' or s=='S') and w=='L':\n",
    "            output_dict['[*PEAKPROM]']+=1\n",
    "        \n",
    "        if hml=='H' and s in {'P','S'}:\n",
    "            output_dict['[*High_Stress]']+=1\n",
    "        if hml=='L' and s==\"U\":\n",
    "            output_dict['[*Low_Unstress]']+=1\n",
    "        \n",
    "        if hml=='H' and mtr == 's':\n",
    "            output_dict['[*High_Strong]']+=1\n",
    "        if hml=='L' and mtr == 'w':\n",
    "            output_dict['[*Low_Weak]']+=1\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(text_str, meter='default_english', num_processes=1, maxlines=MAXLINES):\n",
    "    \"\"\"\n",
    "    Parse the string, assuming line as unit\n",
    "    \"\"\"\n",
    "    # prosodic parse\n",
    "    if maxlines: text_str='\\n'.join(text_str.split('\\n')[:maxlines])\n",
    "    text = p.Text(text_str)\n",
    "    meter = text.get_meter(meter)\n",
    "\n",
    "    out_ld=[]\n",
    "    for i,line in enumerate(text.iparse(meter=meter, num_processes=num_processes)):\n",
    "        line_d=get_data_from_line(line,meter)\n",
    "        if not line_d or not 'score_viols' in line_d: continue\n",
    "        line_d['line_id']=i+1\n",
    "        out_ld.append(line_d)\n",
    "    return out_ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.DataFrame(parse_string(\"\"\"With what attractive charms this goodly frame \n",
    "# Of nature touches the consenting hearts \n",
    "# Of mortal men; and what the pleasing stores \n",
    "# Which beauteous imitation thence derives \n",
    "# To deck the poet's, or the painter's toil; \n",
    "# My verse unfolds.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By line in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_by_line(path_to_txt_or_xml_file, meter='default_english', num_processes=1):\n",
    "    # get txt\n",
    "    txt=read_file(path_to_txt_or_xml_file)\n",
    "    \n",
    "    # return parse\n",
    "    return parse_string(txt, meter=meter, num_processes=num_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.DataFrame(parse_by_line(eg_path)).sort_values('score_viols').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_window1=pd.DataFrame(parse_by_line(eg_path1))\n",
    "# df_window2=pd.DataFrame(parse_by_line(eg_path2))\n",
    "# print(df_window1.mean()['score_viols'], df_window2.mean()['score_viols'])\n",
    "# df_window2.sort_values('num_viols').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice(l,num_slices=None,slice_length=None,runts=True,random=False):\n",
    "    \"\"\"\n",
    "    Returns a new list of n evenly-sized segments of the original list\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        import random\n",
    "        random.shuffle(l)\n",
    "    if not num_slices and not slice_length: return l\n",
    "    if not slice_length: slice_length=int(len(l)/num_slices)\n",
    "    newlist=[l[i:i+slice_length] for i in range(0, len(l), slice_length)]\n",
    "    if runts: return newlist\n",
    "    return [lx for lx in newlist if len(lx)==slice_length]\n",
    "\n",
    "def ngram(l,n=3):\n",
    "    grams=[]\n",
    "    gram=[]\n",
    "    for x in l:\n",
    "        gram.append(x)\n",
    "        if len(gram)<n: continue\n",
    "        g=tuple(gram)\n",
    "        grams.append(g)\n",
    "        gram.reverse()\n",
    "        gram.pop()\n",
    "        gram.reverse()\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice(read_file(eg_path).split(), slice_length=5)\n",
    "#len(ngram(read_file(eg_path).split(), n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_by_window(path_to_txt_or_xml_file, meter='default_english', window_size=5,overlapping_windows=False,max_slices=100000,num_processes=1,):\n",
    "    # get txt\n",
    "    txt=read_file(path_to_txt_or_xml_file)\n",
    "    words=txt.split()\n",
    "    \n",
    "    if overlapping_windows:\n",
    "        word_slices = ngram(words,n=window_size)\n",
    "    else:\n",
    "        word_slices = slice(words,slice_length=window_size)\n",
    "        word_slices = word_slices[:max_slices]\n",
    "        \n",
    "    txt = '\\n'.join([' '.join(slicex) for slicex in word_slices])\n",
    "    \n",
    "    return parse_string(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_window1=pd.DataFrame(parse_by_window(eg_path1))\n",
    "# df_window2=pd.DataFrame(parse_by_window(eg_path2))\n",
    "# print(df_window1.mean()['score_viols'], df_window2.mean()['score_viols'])\n",
    "# df_window2.sort_values('num_viols').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires NLTK\n",
    "def parse_by_phrase(path_to_txt_or_xml_file, meter='default_english', minword=5):\n",
    "    # get txt\n",
    "    txt=read_file(path_to_txt_or_xml_file)\n",
    "    \n",
    "    # phrases\n",
    "    import re\n",
    "    phrases=re.split('[?.,;:\\n]', txt)\n",
    "    \n",
    "    # recombine for minword\n",
    "    if minword:\n",
    "        phrases2=[]\n",
    "        phrase=[]\n",
    "        for px in phrases:\n",
    "            phrase+=px.split()\n",
    "            if len(phrase)>=minword:\n",
    "                phrases2+=[' '.join(phrase)]\n",
    "                phrase=[]\n",
    "        phrases=phrases2\n",
    "    \n",
    "    # make txt\n",
    "    txt = '\\n'.join(phrases)\n",
    "\n",
    "    # return parsed\n",
    "    return parse_string(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_window1=pd.DataFrame(parse_by_phrase(eg_path1))\n",
    "# df_window2=pd.DataFrame(parse_by_phrase(eg_path2))\n",
    "# print(df_window1.mean()['score_viols'], df_window2.mean()['score_viols'])\n",
    "# df_window2.sort_values('score_viols',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
