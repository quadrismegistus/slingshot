{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions available for slingshot\n",
    "STONES = ['ner_spacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy_string(string):\n",
    "    \"\"\"\n",
    "    Using spacy, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # import spacy\n",
    "        import spacy\n",
    "        #import nltk\n",
    "    except ImportError:\n",
    "        print(\"spacy not installed. Please follow directions above.\")\n",
    "        return\n",
    "\n",
    "    # clean string\n",
    "    string = string.strip().replace(\"’\",\"'\").replace(\"‘\",\"'\").replace('—',' -- ').replace('\\r\\n','\\n').replace('\\r','\\n')\n",
    "    \n",
    "    # load its default English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # make an output list\n",
    "    output_list = []\n",
    "    \n",
    "    sent_num=0\n",
    "    \n",
    "    # split at pargraphs:\n",
    "    paragraphs=string.split('\\n\\n')\n",
    "    for para_i,para in enumerate(paragraphs):\n",
    "        para=para.strip()\n",
    "        #if para_i and not para_i%1000: print(para_i,'of',len(paragraphs),'paragraphs')\n",
    "        if para_i and not para_i%1000: print(para_i,'/',len(paragraphs),'paras')\n",
    "    \n",
    "        # create a spacy text object\n",
    "        try:\n",
    "            doc = nlp(para,disable=['tagger'])\n",
    "        except ValueError:\n",
    "            # in case too big for spacy to handle\n",
    "            continue\n",
    "\n",
    "        # loop over sentences\n",
    "        \n",
    "        #for sent in doc.sents:\n",
    "        #sents=nltk.sent_tokenize(string)\n",
    "        sents=doc.sents\n",
    "        for sent_doc in sents:\n",
    "            #sent_doc=nlp(sent, disable=['parser','tagger'])\n",
    "            #if sent_num and not sent_num%1000: print(sent_num)\n",
    "\n",
    "            sent_num+=1\n",
    "            added_sent_already = False\n",
    "\n",
    "            # loop over sentence's entities\n",
    "            #sent_doc = nlp(str(sent))\n",
    "            for ent in sent_doc.ents:\n",
    "\n",
    "                # make a result dict\n",
    "                result_dict = {}\n",
    "\n",
    "                # set sentence number\n",
    "                #result_dict['_para_num'] = para_i+1\n",
    "                result_dict['_sent_num'] = sent_num\n",
    "\n",
    "                # store text too\n",
    "                if not added_sent_already:\n",
    "                    sent=sent_doc.text\n",
    "                    result_dict['_sent'] = sent\n",
    "                    added_sent_already = True\n",
    "                else:\n",
    "                    result_dict['_sent'] = ''\n",
    "\n",
    "                # get type\n",
    "                result_dict['type'] = ent.label_\n",
    "\n",
    "                # get entity\n",
    "                result_dict['entity'] = ent.text\n",
    "\n",
    "                # get start char\n",
    "                result_dict['start_char'] = ent.start_char\n",
    "\n",
    "                # get end char\n",
    "                result_dict['end_char'] = ent.end_char\n",
    "\n",
    "                # add result_dict to output_list\n",
    "                output_list.append(result_dict)\n",
    "            \n",
    "    # return output\n",
    "    return output_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner_spacy_string(\"I'm on my way to the Grand Hotel Abyss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy(path_to_txt_file):\n",
    "    print(path_to_txt_file)\n",
    "    try:\n",
    "        with open(path_to_txt_file) as file:\n",
    "            txt=file.read()\n",
    "        return ner_spacy_string(txt)\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = ner_spacy('/Users/ryan/literarytextmining/corpora/fiction_since_1990/texts/Brown,_Dan.The_Da_Vinci_Code.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.DataFrame(results[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
